{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Modeling of toxic subtype labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier, AdaBoostClassifier\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "import sys \n",
    "from textblob import TextBlob, Word\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "pd.set_option('display.max_rows', 100) # to look at more rows of data later\n",
    "pd.set_option('display.max_columns', 100) # to expand columns view so that all can be seen later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../dataset/positiveset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['severe_toxicity','obscene','threat','insult','identity_attack','sexual_explicit']\n",
    "for i in col:\n",
    "    data[i] = [1 if j >= 0.5 else 0 for j in data[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    123441\n",
      "1        13\n",
      "Name: severe_toxicity, dtype: int64\n",
      "0    115772\n",
      "1      7682\n",
      "Name: obscene, dtype: int64\n",
      "0    119942\n",
      "1      3512\n",
      "Name: threat, dtype: int64\n",
      "1    89197\n",
      "0    34257\n",
      "Name: insult, dtype: int64\n",
      "0    112450\n",
      "1     11004\n",
      "Name: identity_attack, dtype: int64\n",
      "0    120104\n",
      "1      3350\n",
      "Name: sexual_explicit, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for name in col:\n",
    "    print(data[name].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will classify 'Severe Toxicity', 'Threat' and 'Sexual Explicit' as a separate category 'Others' since number of positives are less than 5% of the total rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditions(data):\n",
    "    if (data['severe_toxicity'] == 1) or (data['threat'] == 1) or (data['sexual_explicit'] == 1):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['others'] = data.apply(conditions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6799"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['others'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['comment'],\n",
    "                                                    data[['obscene','insult','identity_attack','others']],\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'cvec': CountVectorizer(),\n",
    "    'tvec': TfidfVectorizer(),\n",
    "    'lr': LogisticRegression(solver='lbfgs'),\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'nb': MultinomialNB(),\n",
    "    'rf': RandomForestClassifier(),\n",
    "    'et': ExtraTreesClassifier(),\n",
    "    'ada': AdaBoostClassifier(random_state=42),\n",
    "    'ovr_lr': OneVsRestClassifier(LogisticRegression(solver='lbfgs',max_iter=300)),\n",
    "    'clf_lr': ClassifierChain(LogisticRegression(solver='lbfgs',max_iter=300),random_state=42,order='random'),\n",
    "    'clf_ada': ClassifierChain(AdaBoostClassifier())\n",
    "}\n",
    "\n",
    "model_full = {\n",
    "    'cvec': 'CountVectorizer',\n",
    "    'tvec': 'TfidfVectorizer',\n",
    "    'lr': 'Logistic Regression',\n",
    "    'knn': 'KNearestNeighbor',\n",
    "    'nb': 'Multinomial NB',\n",
    "    'dt': 'Decision Tree',\n",
    "    'rf': 'Random Forest',\n",
    "    'et': 'Extra Tree',\n",
    "    'ada': 'AdaBoost',\n",
    "    'ovr_lr': 'OneVsRest (Logistic Regression)',\n",
    "    'clf_lr': 'Classifier Chain (Logistic Regression)',\n",
    "    'clf_ada': 'Classifier Chain (Adaboost)'\n",
    "}\n",
    "\n",
    "param_dict = {\n",
    "    'cvec': {\n",
    "        'cvec__max_features': [5000,6000,7000],\n",
    "        'cvec__min_df': [3,4],\n",
    "        'cvec__max_df': [.9, .95],\n",
    "        'cvec__ngram_range': [(1,1), (1,2)]\n",
    "    },\n",
    "    'tvec': {\n",
    "        'tvec__max_features': [5000,6000,7000],\n",
    "        'tvec__min_df': [3,4],\n",
    "        'tvec__max_df': [.9, .95],\n",
    "        'tvec__ngram_range': [(1,1), (1,2)]\n",
    "    },\n",
    "    'knn': {\n",
    "        'knn__n_neighbors': [5,6,7,8,9]\n",
    "    },\n",
    "    'lr': {},\n",
    "    'nb': {},\n",
    "    'dt': {\n",
    "        'dt__max_depth': [5,7],\n",
    "        'dt__min_samples_split': [10,15],\n",
    "        'dt__min_samples_leaf': [3,4]\n",
    "    },\n",
    "    'rf': {\n",
    "        'rf__n_estimators': [100],\n",
    "        'rf__max_depth': [5,7],\n",
    "        'rf__min_samples_split': [10,15],\n",
    "        'rf__min_samples_leaf': [3,4]\n",
    "        \n",
    "    },\n",
    "    'et': {\n",
    "        'et__n_estimators': [100],\n",
    "        'et__max_depth': [5,7],\n",
    "        'et__min_samples_split': [10,15],\n",
    "        'et__min_samples_leaf': [3,4]\n",
    "    },\n",
    "    'ada': {\n",
    "        'ada__n_estimators': [50,100,200],\n",
    "        'ada__learning_rate': [0.9, 1]\n",
    "    },\n",
    "    'ovr_lr': {},\n",
    "    'clf_lr': {},\n",
    "    'clf_ada': {}\n",
    "}\n",
    "\n",
    "def prepare_pipeline(list_of_models):\n",
    "    \"\"\"\n",
    "    Prepare pipeline of models to be used for modelling\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_models: list[str]\n",
    "        List of models to be included for pipeline\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Pipeline\n",
    "        Pipeline of models to be run\n",
    "    \"\"\"\n",
    "    pipe_list = [(i,model_dict[i]) for i in list_of_models]\n",
    "    return Pipeline(pipe_list)\n",
    "\n",
    "def add_params(name,pipe_dict):\n",
    "    \"\"\"\n",
    "    Add parameters for GridSearch\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name: str\n",
    "        Name of model/vectorization method to have params added.\n",
    "    pipe_dict: Dictionary\n",
    "        Dictionary that contains parameters to be added into GridSearch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary\n",
    "        Dictionary that contains parameters to be added for GridSearch\n",
    "    \"\"\"\n",
    "    params = param_dict[name]\n",
    "    for k,v in params.items():\n",
    "        pipe_dict[k] = v\n",
    "    return pipe_dict\n",
    "\n",
    "def grid_search(vec_method,model,filename,X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test):\n",
    "    \"\"\"\n",
    "    Initialize and run GridSearch\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec_method: str\n",
    "        Vectorization method to use. Vectorization method has to be contained in model_dict.\n",
    "        \n",
    "    model: str\n",
    "        Initialize which classification model to use. Note classification model has to be contained in model_dict.\n",
    "        \n",
    "    filename: str\n",
    "        Name of pickle file to save to.\n",
    "        \n",
    "    X_train: list[str]\n",
    "        List of training data to be used\n",
    "        \n",
    "    y_train: list[str]\n",
    "        Target value of the training data\n",
    "        \n",
    "    X_test: list[str]\n",
    "        List of test data to be used \n",
    "        \n",
    "    y_test: list[str]\n",
    "        Target value of test data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List\n",
    "        List that contains predicted values of the test data\n",
    "    \"\"\"\n",
    "    pipe_params = {}\n",
    "    pipe_params = add_params(vec_method,pipe_params)\n",
    "    pipe_params = add_params(model,pipe_params)\n",
    "    pipe = prepare_pipeline([vec_method,model])\n",
    "    gs = GridSearchCV(pipe,param_grid=pipe_params,cv=3,n_jobs=3)\n",
    "    gs.fit(X_train,y_train)\n",
    "    print(f'Using {model_full[model]} with {model_full[vec_method]}:')\n",
    "    print(f'Train Score: {round(gs.best_score_,4)}')\n",
    "    print(f'Test Score: {round(gs.score(X_test,y_test),4)}')\n",
    "    print(f'Using the following parameters: {gs.best_params_}')\n",
    "    # Save model into pickle\n",
    "    pickle.dump(gs, open(filename, 'wb'))\n",
    "    return gs.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OneVsRest (Logistic Regression) with CountVectorizer:\n",
      "Train Score: 0.7047\n",
      "Test Score: 0.7114\n",
      "Using the following parameters: {'cvec__max_df': 0.9, 'cvec__max_features': 5000, 'cvec__min_df': 4, 'cvec__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "ovrlr_cvec_predictions = grid_search('cvec','ovr_lr','cvec_ovrlr.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OneVsRest (Logistic Regression) with TfidfVectorizer:\n",
      "Train Score: 0.7228\n",
      "Test Score: 0.7285\n",
      "Using the following parameters: {'tvec__max_df': 0.9, 'tvec__max_features': 6000, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "ovrlr_tvec_predictions = grid_search('tvec','ovr_lr','tvec_ovrlr.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Classifier Chain (Logistic Regression) with CountVectorizer:\n",
      "Train Score: 0.7068\n",
      "Test Score: 0.7128\n",
      "Using the following parameters: {'cvec__max_df': 0.9, 'cvec__max_features': 5000, 'cvec__min_df': 3, 'cvec__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "clflr_cvec_predictions = grid_search('cvec','clf_lr','cvec_clflr.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Classifier Chain (Logistic Regression) with TfidfVectorizer:\n",
      "Train Score: 0.7246\n",
      "Test Score: 0.7312\n",
      "Using the following parameters: {'tvec__max_df': 0.9, 'tvec__max_features': 6000, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "clflr_tvec_predictions = grid_search('tvec','clf_lr','tvec_clflr.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Classifier Chain (Adaboost) with TfidfVectorizer:\n",
      "Train Score: 0.7019\n",
      "Test Score: 0.7042\n",
      "Using the following parameters: {'tvec__max_df': 0.9, 'tvec__max_features': 6000, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "clfada_tvec_predictions = grid_search('tvec','clf_ada','tvec_clfada.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Classifier Chain (Adaboost) with CountVectorizer:\n",
      "Train Score: 0.6946\n",
      "Test Score: 0.6992\n",
      "Using the following parameters: {'cvec__max_df': 0.9, 'cvec__max_features': 5000, 'cvec__min_df': 3, 'cvec__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "clfada_cvec_predictions = grid_search('cvec','clf_ada','cvec_clfada.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(loaded_model.predict(X_test),columns=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        obscene       0.68      0.49      0.57      1877\n",
      "         insult       0.86      0.90      0.88     22491\n",
      "identity_attack       0.71      0.48      0.57      2724\n",
      "         others       0.58      0.33      0.42      1678\n",
      "\n",
      "      micro avg       0.83      0.80      0.81     28770\n",
      "      macro avg       0.71      0.55      0.61     28770\n",
      "   weighted avg       0.81      0.80      0.80     28770\n",
      "    samples avg       0.72      0.70      0.71     28770\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carol\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Carol\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, ovrlr_cvec_predictions, target_names=['obscene','insult','identity_attack','others']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        obscene       0.76      0.36      0.49      1877\n",
      "         insult       0.85      0.93      0.89     22491\n",
      "identity_attack       0.75      0.49      0.60      2724\n",
      "         others       0.72      0.28      0.40      1678\n",
      "\n",
      "      micro avg       0.84      0.81      0.82     28770\n",
      "      macro avg       0.77      0.51      0.59     28770\n",
      "   weighted avg       0.83      0.81      0.80     28770\n",
      "    samples avg       0.74      0.72      0.72     28770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, ovrlr_tvec_predictions, target_names=['obscene','insult','identity_attack','others']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        obscene       0.75      0.35      0.47      1877\n",
      "         insult       0.85      0.93      0.89     22491\n",
      "identity_attack       0.75      0.52      0.61      2724\n",
      "         others       0.70      0.36      0.47      1678\n",
      "\n",
      "      micro avg       0.83      0.82      0.83     28770\n",
      "      macro avg       0.76      0.54      0.61     28770\n",
      "   weighted avg       0.82      0.82      0.81     28770\n",
      "    samples avg       0.75      0.72      0.73     28770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clflr_tvec_predictions, target_names=['obscene','insult','identity_attack','others']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        obscene       0.67      0.49      0.56      1877\n",
      "         insult       0.86      0.90      0.88     22491\n",
      "identity_attack       0.70      0.49      0.58      2724\n",
      "         others       0.57      0.38      0.46      1678\n",
      "\n",
      "      micro avg       0.82      0.80      0.81     28770\n",
      "      macro avg       0.70      0.56      0.62     28770\n",
      "   weighted avg       0.81      0.80      0.80     28770\n",
      "    samples avg       0.72      0.71      0.71     28770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clflr_cvec_predictions, target_names=['obscene','insult','identity_attack','others']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        obscene       0.70      0.48      0.57      1877\n",
      "         insult       0.80      0.95      0.87     22491\n",
      "identity_attack       0.70      0.47      0.56      2724\n",
      "         others       0.55      0.17      0.26      1678\n",
      "\n",
      "      micro avg       0.78      0.83      0.80     28770\n",
      "      macro avg       0.69      0.52      0.56     28770\n",
      "   weighted avg       0.77      0.83      0.78     28770\n",
      "    samples avg       0.75      0.73      0.73     28770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clfada_tvec_predictions, target_names=['obscene','insult','identity_attack','others']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        obscene       0.75      0.40      0.52      1877\n",
      "         insult       0.80      0.95      0.87     22491\n",
      "identity_attack       0.69      0.36      0.47      2724\n",
      "         others       0.49      0.13      0.20      1678\n",
      "\n",
      "      micro avg       0.79      0.81      0.80     28770\n",
      "      macro avg       0.68      0.46      0.52     28770\n",
      "   weighted avg       0.77      0.81      0.77     28770\n",
      "    samples avg       0.73      0.72      0.72     28770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clfada_cvec_predictions, target_names=['obscene','insult','identity_attack','others']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the f1-score as the scoring metric, using classifier class(Logistic Regression) with CountVectorizer is the best models among all the models to be used as the multilabel classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
